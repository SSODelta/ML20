{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 14 Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex1: Basic Text Classification\n",
    "In this exercise you must implement a basic text classification pipeline and apply it to SMS spam classification.\n",
    "The pipeline is\n",
    "\n",
    "* Clean strings and split them into words.\n",
    "    You need to complete the implementation of **standardize_strings**, **split_strings**.\n",
    "* Make vocabulary/dictionary with mappings from word to index and index to word and transforming vectors.\n",
    "    You need to complete the implementation of **make_vocabulary_and_index**, **words_to_vectors**\n",
    "* Apply a standard Logistic Regression classifier on given data and output the results.\n",
    "  You need to complete the implementation of **run_bow_classifier**\n",
    "\n",
    "There is a lot of code and hopefully helpful comments.\n",
    "\n",
    "If the technicalities of this becomes to hard you can move on to the next exercise where we use a standard implementation from sklearn in python to steps 1 and 2 as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING 'standardize_strings': \t\t\tPASSED!\n",
      "TESTING 'split_strings': \t\t\tPASSED!\n",
      "TESTING 'make_vocabulary_and_index': \t\tPASSED!\n",
      "TESTING 'words_to_vector': \t\t\tPASSED!\n",
      "['Probably not, still going over some stuff here\\n', 'I HAVE A DATE ON SUNDAY WITH WILL!!\\n', 'Thanks 4 your continued support Your question this week will enter u in2 our draw 4 Â£100 cash. Name the NEW US President? txt ans to 80082\\n', \"Dear 0776xxxxxxx U've been invited to XCHAT. This is our final attempt to contact u! Txt CHAT to 86688 150p/MsgrcvdHG/Suite342/2Lands/Row/W1J6HL LDN 18yrs\\n\", 'I sent my scores to sophas and i had to do secondary application for a few schools. I think if you are thinking of applying, do a research on cost also. Contact joke ogunrinde, her school is one me the less expensive ones\\n', 'Kothi print out marandratha.\\n', 'Arun can u transfr me d amt\\n', 'I asked you to call him now ok\\n', 'Ringtone Club: Gr8 new polys direct to your mobile every week !\\n', 'Hello! Just got here, st andrews-boy its a long way! Its cold. I will keep you posted\\n']\n",
      "train mean accuracy 0.9970540974825924, test mean accuracy 0.9836956521739131\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    #wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "    with open('smsspamcollection/SMSSpamCollection', 'r') as f:\n",
    "        dat = f.readlines()\n",
    "    labels = [x.split('\\t')[0] for x in dat]\n",
    "    texts = [x.split('\\t')[1] for x in dat]\n",
    "    sl = set(labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.33, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def test_vectorize():\n",
    "    \"\"\" \n",
    "    test words_to_vectors \n",
    "    \"\"\"\n",
    "\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxz'\n",
    "    vocab = {x for x in alphabet}\n",
    "    index_map = {x: i for (i, x) in enumerate(sorted(vocab))}\n",
    "    a = [1, 2]\n",
    "    b = [5]\n",
    "    c = [0, 10]\n",
    "    index_lists = [a, b, c]\n",
    "    word_lists = [[alphabet[i] for i in x] for x in index_lists]\n",
    "    \n",
    "    print(\"TESTING 'make_vocabulary_and_index': \\t\\t\", end='')\n",
    "    learned_vocab, word_to_index, index_to_word = make_vocabulary_and_index(word_lists)\n",
    "    target_vocab = {'a', 'b', 'c', 'f', 'k'}    \n",
    "    \n",
    "    assert target_vocab == learned_vocab, \"Error with 'make_vocabulary_and_index'.\\nExpected vocabulary: \\t{0}\\n Got: \\t\\t{1}\".format(target_vocab, learned_vocab)\n",
    "    # Test word_to_index and index_to_word by checking they are inverse mappings. \n",
    "    # inv_map = {v: k for k, v in tc.word_to_index.items()} \n",
    "    # assert tc.index_to_word == inv_map, \"Error with 'make_vocabulary_and_index'.\\nExpected that the dictionaries 'index_to_word' and 'word_to_index' are each others inverse. This was not the case. \"\n",
    "    print(\"PASSED!\")\n",
    "\n",
    "    print(\"TESTING 'words_to_vector': \\t\\t\\t\", end='')\n",
    "    string_vectors = words_to_vectors(word_lists, vocabulary=vocab, index_map=word_to_index)\n",
    "    def my_idx(sid, wid):\n",
    "        return word_to_index[word_lists[sid][wid]]\n",
    "    target = np.zeros((3, len(vocab)))\n",
    "    target[0, my_idx(0, 0)] = 1\n",
    "    target[0, my_idx(0, 1)] = 1\n",
    "    target[1, my_idx(1, 0)] = 1\n",
    "    target[2, my_idx(2, 0)] = 1\n",
    "    target[2, my_idx(2, 1)] = 1\n",
    "    assert np.allclose(string_vectors, target),  \"Error with words to vectors\"\n",
    "    \n",
    "    # print(string_vectors-target)\n",
    "    print('PASSED!')\n",
    "\n",
    "def test_string_cleaning():\n",
    "    \"\"\" \n",
    "    Test string cleaning\n",
    "    \"\"\"\n",
    "    strings = ['i THINK machine LEARNING is cool.,-()', 'he likes cake cake cake',\n",
    "               'pandora has a box that should not be opened']\n",
    "\n",
    "    bad_words = {'i', 'the', 'has', 'he', 'has', 'a', 'is'}\n",
    "    result = [['think', 'machine', 'learning', 'cool'], ['likes', 'cake', 'cake', 'cake'],\n",
    "              ['pandora', 'box', 'that', 'should', 'not', 'be', 'opened']]\n",
    "\n",
    "    print(\"TESTING 'standardize_strings': \\t\\t\\t\", end='')\n",
    "    expect = ['i think machine learning is cool', 'he likes cake cake cake', 'pandora has a box that should not be opened']\n",
    "    strings_cleaned = standardize_strings(strings) # lower string remove special characters\n",
    "    assert strings_cleaned == expect, \"Error in 'standardize_strings'.\\nExpected \\t{0}\\nGot \\t\\t{1}\".format(expect, strings_cleaned)\n",
    "    print('PASSED!')\n",
    "\n",
    "    print(\"TESTING 'split_strings': \\t\\t\\t\", end='')\n",
    "    expect = [['i', 'think', 'machine', 'learning', 'is', 'cool'], ['he', 'likes', 'cake', 'cake', 'cake'], ['pandora', 'has', 'a', 'box', 'that', 'should', 'not', 'be', 'opened']]\n",
    "    word_lists = split_strings(strings_cleaned)\n",
    "    assert word_lists == expect, \"Error in 'split_strings': \\nExpected\\t{0}\\nGot \\t\\t{1}\".format(expect, word_lists)\n",
    "    print(\"PASSED!\")\n",
    "    \n",
    "    \n",
    "def standardize_strings(string_list):\n",
    "    \"\"\" \n",
    "    Standardize strings by \n",
    "        1. making them lower case (example 'LaTeX' -> 'latex')\n",
    "        2. remove any non-alphabetic characters, i.e. \n",
    "           characters not in [a-z]. (example \"l33t_h@x\" -> \"lthx\")\n",
    "\n",
    "    For example:\n",
    "        >>> standardize_strings(['remove . please', 'also .,-_()'])\n",
    "        ['remove  please', 'also ']\n",
    "    \n",
    "    The following methods might be useful: \n",
    "        str.lower,\n",
    "        re.sub (regular expression from re package), \n",
    "        str.replace, \n",
    "\n",
    "\n",
    "    You can ignore irrelevant spacing since we'll take care of it later. \n",
    "\n",
    "    Args:\n",
    "    string_list: list of strings\n",
    "    \n",
    "    Returns:\n",
    "    list of strings without non-alphabetic characters\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    \n",
    "    ### YOUR CODE 1-3 lines\n",
    "    lower = [x.lower() for x in string_list]\n",
    "    res = [re.sub('[^a-z ]', '', x) for x in lower]\n",
    "    ### END CODE\n",
    "    \n",
    "    assert len(string_list) == len(res)\n",
    "    return res\n",
    "\n",
    "def split_strings(strings):\n",
    "    \"\"\" \n",
    "    Split a list of strings into list of list of words. The splitting should be done on space ' '.\n",
    "\n",
    "    For example: \n",
    "        >>> split_strings(['split me please', 'me to'])\n",
    "        [['split', 'me', 'please'], ['me', 'to']]\n",
    "        \n",
    "    Try to use list comprehension instead of writing function with loops or recursion. \n",
    "    List comprehension works as follows\n",
    "\n",
    "        >>> dat = list(range(6)) \n",
    "        >>> dat\n",
    "        [0,1,2,3,4,5]\n",
    "\n",
    "        >>> dat_squared = [x**2 for x in dat] \n",
    "        >>> dat_squared\n",
    "        [0, 1, 4, 9, 16, 25]\n",
    "\n",
    "    The following method might be useful:\n",
    "        - str.split()  (for help with splitting strings)\n",
    "\n",
    "    Args:\n",
    "    strings: list of strings\n",
    "\n",
    "    Returns:\n",
    "    list of lists of strings (words)\n",
    "    \"\"\"\n",
    "    word_lists = []\n",
    "    \n",
    "    ### YOUR CODE \n",
    "    word_lists = [[x for x in string.split(' ')] for string in strings]\n",
    "    ### END CODE\n",
    "    \n",
    "    assert len(word_lists) == len(strings)\n",
    "    return word_lists\n",
    "\n",
    "\n",
    "\n",
    "def make_vocabulary_and_index(word_lists):\n",
    "    \"\"\"\n",
    "    Compute the vocabulary (set), word_to_index (dict), and index_to_word\n",
    "    for the classifier.\n",
    "     - vocabulary must be a set with all words in the list of lists of words word_lists\n",
    "     - word_to_index is a dictionary that maps each word to a unique index\n",
    "     - index_to_word is a dictionary which is the inverse of word_to_index\n",
    "\n",
    "    Use set and dict comprehension  (like list comprehension) to _ each with one list of code\n",
    "    {x for x in [1,2,3,4,5,6]} is a set comprehension which makes a set of elements in [1,2,3,4,5,6]\n",
    "\n",
    "    As an example\n",
    "    If word_lists is [['a', 'b'], ['c','b']] then vocabulary should be {'a', 'b', 'c'} and word_to_index should be something like {'a':0, 'b':1, 'c':2} \n",
    "    (the permutation is irrelevant, this is just the obvious one.) \n",
    "    Also, index_to_word should be in this case {0:'a', 1:'b', 2:'c'}\n",
    "\n",
    "    Args:\n",
    "    word_lists: list of lists of strings\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    word_to_index = dict()\n",
    "    index_to_word = dict()\n",
    "\n",
    "    ### YOUR CODE 3-4 lines\n",
    "    vocabulary = {x for word_list in word_lists for x in word_list}\n",
    "    word_to_index = {x: i for (i, x) in enumerate(vocabulary)}\n",
    "    index_to_word = {i: x for (i, x) in enumerate(vocabulary)}\n",
    "    ### END CODE\n",
    "\n",
    "    assert len(word_to_index) == len(index_to_word)\n",
    "    return vocabulary, word_to_index, index_to_word\n",
    "    \n",
    "def words_to_vectors(word_lists, vocabulary, index_map):\n",
    "    \"\"\"\n",
    "    Each list of strings in word_lists is turned into a vector of length |vocabulary| in such a way that the ith entry of the vector is the number of occurences of word i (under index_map) in the string.\n",
    "    index_map is the dictionary {word : index}. If the word is not in the vocabulary you should ignore it. \n",
    "\n",
    "    As an example\n",
    "    If the vocabulary is {'a','b','c','d','e'} and the mapping index_map is {'a':0,'b':1,'c':2,'d':3,'e':4}, then the word_list ['a','b','a'] is mapped to [2, 1, 0, 0, 0]\n",
    "\n",
    "    A way of doing this is as follows (you can come up with a different implementation as long as the results are the same):\n",
    "    1. Map each word to its index transforming list of words to list of indices\n",
    "    2. Fill the vectors using these indices\n",
    "\n",
    "    Step 1 can be done easily.\n",
    "    For the second step the collections.Counter class may be useful: given a list it returns a dictionary-like object counting the ocurrences of each entry in the list. \n",
    "    As an example of this:\n",
    "    c = Counter([1,1,2]) # -> Counter({1: 2, 2: 1})\n",
    "    Now c.keys, c.values, c.items gives the list of indices and counts\n",
    "    In [1]: list(c.keys())\n",
    "    Out[1]: [1, 2]\n",
    "    In [2]: list(c.values())\n",
    "    Out[2]: [2, 1]\n",
    "\n",
    "    Remember indexing in numpy:\n",
    "    if x is a numpy array and ind a list of indices, then a[ind] indexes into the entries of x given in ind.\n",
    "\n",
    "    Args:\n",
    "    word_lists: list of lists of strings (each string is in self.vocabulary)\n",
    "\n",
    "    Returns: \n",
    "    word_vectors: numpy array of size |word_lists| X |vocabulary|. The ith row is the vector to which the ith word_list is mapped by counting the occurrences of each word.\n",
    "    \"\"\"\n",
    "    word_vectors = np.zeros((len(word_lists), len(vocabulary)))\n",
    "\n",
    "    ### YOUR CODE\n",
    "    index_lists = [[index_map[x] for x in word_list if x in index_map] for word_list in word_lists]\n",
    "    #index_lists = [[index_map[x] for x in word_list] for word_list in word_lists]\n",
    "\n",
    "\n",
    "    counters = [Counter(x) for x in index_lists]\n",
    "    for i, c in enumerate(counters):\n",
    "        indices = [x[0] for x in c.items()]\n",
    "        counts = [x[1] for x in c.items()]\n",
    "        word_vectors[i, indices] = counts\n",
    "    ### END CODE\n",
    "\n",
    "    return word_vectors\n",
    "\n",
    "def data_clean(strings, stopwords = set()):\n",
    "        \"\"\"\n",
    "        Clean the data by calling the string functions you made\n",
    "        Nothing required here\n",
    "        \"\"\"\n",
    "        clean_data = standardize_strings(strings)\n",
    "        word_lists = split_strings(clean_data)\n",
    "        return word_lists\n",
    "    \n",
    "def run_bow_classifier(train_strings, test_strings, train_labels, test_labels):\n",
    "    \"\"\" Transform strings to vectors and run a logistic regression model on it and see the results \n",
    "\n",
    "    You can use the following Logisticregression model (ask google for details if needed). Supports fit and score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    1. transform data, both train and test (use same transform for both)\n",
    "    2. fit Logistic Regression model\n",
    "    3. Print train score and test score\n",
    "    \"\"\"\n",
    "    print(train_strings[0:10])\n",
    "    ### YOUR CODE HERE\n",
    "    clean_train_strings = data_clean(train_strings)\n",
    "    clean_test_strings = data_clean(test_strings)\n",
    "    vocabulary, word_to_index, index_to_word = make_vocabulary_and_index(clean_train_strings)\n",
    "    \n",
    "    X_train = words_to_vectors(clean_train_strings, vocabulary, word_to_index)\n",
    "    X_test = words_to_vectors(clean_test_strings, vocabulary, word_to_index)\n",
    "    clf = LogisticRegression(random_state=0).fit(X_train, train_labels)\n",
    "    train_score =  clf.score(X_train, train_labels)\n",
    "    test_score = clf.score(X_test, test_labels)\n",
    "    print(f'train mean accuracy {train_score}, test mean accuracy {test_score}')\n",
    "    \n",
    "    ### END CODE\n",
    "  \n",
    "test_string_cleaning()\n",
    "test_vectorize()  \n",
    "run_bow_classifier(*get_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2: TFIDF Using standard tools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See here for a default implementation of transforming text to TFIDF transformed vectors\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "\n",
    "Re do the text classification from above using the TfidfVectorizer class.\n",
    "You can play with the parameters if you like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mean accuracy 0.998393144081414, test mean accuracy 0.9820652173913044\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def run_tfidf_classifier(train_strings, test_strings, train_labels, test_labels):\n",
    "    \"\"\" Transform strings to vectors and run a logistic regression model on it and see the results \n",
    "\n",
    "    You can use the following Logisticregression model (ask google for details if needed). Supports fit and score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    1. transform data, both train and test using TFIDF transform\n",
    "    2. fit Logistic Regression model\n",
    "    3. Print train score and test score\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    #vectorizer = CountVectorizer()\n",
    "    #train_strings = standardize_strings(train_strings)\n",
    "    #test_strings = standardize_strings(test_strings)\n",
    "\n",
    "    vectorizer.fit(train_strings)\n",
    "    X_train = vectorizer.transform(train_strings)\n",
    "    X_test = vectorizer.transform(test_strings)\n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    clf.fit(X_train, train_labels)\n",
    "    train_score =  clf.score(X_train, train_labels)\n",
    "    test_score = clf.score(X_test, test_labels)\n",
    "    print(f'train mean accuracy {train_score}, test mean accuracy {test_score}')\n",
    "    ### END CODE\n",
    "\n",
    "\n",
    "run_tfidf_classifier(*get_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex3: Analyzing Feature Hashing\n",
    "In feature hashing, we map a vector $x \\in R^d$ to a vector in $R^k$ using two hash functions $h : [d] \\to [k]$ and $g : [d] \\to \\{-1,1\\}$. The hash functions are chosen randomly and independently before seeing any data. Assume the hash functions satisfy the following two properties:\n",
    "1. For any two distinct coordinates $i \\neq j$, we have that $g(i)$ and $g(j)$ are independent and uniform random, i.e. for any $a,b \\in \\{-1,1\\}$ it holds that $\\Pr_g[g(i)=a \\wedge g(j)=b] = 1/4$.\n",
    "2. For any two distinct coordinates $i \\neq j$, we have that $\\Pr_h[h(i)=h(j)] \\leq 1/k$.\n",
    "\n",
    "The embedding $f(x)$ of a vector $x$ is obtained by hashing each index $i \\in [d]$ to the index $h(i)$ and adding $g(i) \\cdot x_i$ to $f(x)_{h(i)}$.\n",
    "\n",
    "Your task it to prove:\n",
    "1. For two vectors $x,y$, we have $\\mathbb{E}[f(x)^\\intercal f(y)] = x^\\intercal y$.\n",
    "\n",
    "Hint: The following re-writing may be useful:\n",
    "$$\n",
    "f(x)^\\intercal f(y) = \\sum_{i=1}^d \\sum_{j=1}^d 1_{[h(i)=h(j)]} x_i y_j g(i) g(j),\n",
    "$$\n",
    "where $1_{[h(i)=h(j)]}$ is the indicator random variable taking the value $1$ if $h(i)=h(j)$ and $0$ otherwise.\n",
    "You may also need linearity of expectation $\\mathbb{E}[A + B] = \\mathbb{E}[A] + \\mathbb{E}[B]$ and that for independent random variables $X,Y$ we have $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$.\n",
    "\n",
    "### BEGIN MATH SOLUTION\n",
    "1. Using the hint and linearity of expectation, we see that\n",
    "$$\n",
    "\\mathbb{E}[f(x)^\\intercal f(y)] = \\sum_{i=1}^d \\sum_{j=1}^d \\mathbb{E}[1_{[h(i)=h(j)]} x_i y_j g(i) g(j)]\n",
    "$$\n",
    "Using that $h$ and $g$ are chosen independently and that $g(i)$ is independent of $g(j)$ when $i \\neq j$, it holds for any term with $i \\neq j$ that $\\mathbb{E}[1_{[h(i)=h(j)]} x_i y_j g(i) g(j)] = \\mathbb{E}[1_{[h(i)=h(j)]}] x_i y_j \\mathbb{E}[g(i)] \\mathbb{E}[g(j)]$. Since $g(i)$ is uniform random among $\\{-1,1\\}$, its expectation is $0$ and the whole term is $0$. Therefore we have\n",
    "$$\n",
    "\\mathbb{E}[f(x)^\\intercal f(y)] = \\sum_{i=1}^d \\mathbb{E}[1_{[h(i)=h(i)]} x_i y_j g(i)^2]\n",
    "$$\n",
    "The indicator $1_{[h(i)=h(i)]}$ is always $1$ and $g(i)^2$ is also $1$. Hence\n",
    "$$\n",
    "\\mathbb{E}[f(x)^\\intercal f(y)] = \\sum_{i=1}^d x_i y_i.\n",
    "$$\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Skip Gram and test\n",
    "In this exercise you must implement the Skip Gram Architecture.\n",
    "We have written all the other code required to test a Basic Skip Gram Model. Still Training is rather time consuming.\n",
    "\n",
    "So all you need is to finish the forward method that given an input batch computes the output of the skipgram model (without performing the softmax transformation as that is included in the loss function we use).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.utils.data as torch_data\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import bidict\n",
    "import json\n",
    "\n",
    "class BasicSkipGram(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        \"\"\" Trivial init\n",
    "        \"\"\"\n",
    "        super(BasicSkipGram, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = None # untrained\n",
    "        self.lossfunction = nn.CrossEntropyLoss()\n",
    "\n",
    "      \n",
    "\n",
    "    def forward(self, inputs, params):\n",
    "        \"\"\" Compute the \"forward pass\" i.e the logit for each word in the vocabulary \"\"\"\n",
    "        embedding_mat = params['embedding'] # num_embeddings x embedding_dim\n",
    "        soft_layer = params['soft_layer']\n",
    "        out = None\n",
    "        ### YOUR CODE HERE\n",
    "        embeddings = embedding_mat[inputs, :] # n x embedding_dim\n",
    "        out = embeddings @ soft_layer\n",
    "        ### END CODE\n",
    "        \n",
    "\n",
    "        return out\n",
    "\n",
    "    def loss(self, pred, labels):\n",
    "        return self.lossfunction(pred, labels)\n",
    "    \n",
    "    def train(self, train_loader,  epochs=1):\n",
    "        \"\"\" fit the neural net using BCEWithLogitsLoss i.e. logistic loss of sigmoid(x), y \"\"\"\n",
    "        print('start training emb model')\n",
    "        ## Initialize parameters\n",
    "        train_embedding = torch.randn(net.num_embeddings, net.embedding_dim, requires_grad=True)\n",
    "        soft_layer = torch.randn(net.embedding_dim, net.num_embeddings, requires_grad=True)\n",
    "        params={'embedding': train_embedding, 'soft_layer': soft_layer}\n",
    "\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)\n",
    "        ## Create GD optimizer + Adam is your friend\n",
    "        optimizer = optim.Adam(params.values())\n",
    "        for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "            running_loss = 0.0\n",
    "            running_count = 0\n",
    "            for i, data in enumerate(train_loader):\n",
    "                inputs, labels = data\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = net.forward(inputs, params)\n",
    "                loss = self.loss(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                running_count += len(labels)\n",
    "\n",
    "                if i % 300 == 299:\n",
    "                    mean_loss = running_loss / running_count\n",
    "                    print(f'epoch {epoch}: batch {i+1} mean last 300 loss: {mean_loss} \\r', end='')\n",
    "                    running_loss = 0.0\n",
    "                    running_count = 0\n",
    "            print(f'Epoch {epoch} finished')\n",
    "        print('\\nFinished Training')\n",
    "        self.embedding = train_embedding.detach()\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "def load_real_skipgram_data(path, data_size=-1):\n",
    "    \"\"\" Loads the skip gram data\"\"\"\n",
    "    with open(path, \"r\") as in_file:\n",
    "        data = json.load(in_file)\n",
    "    word_idx_mapping = bidict.bidict(data[\"dictionary\"])\n",
    "    if data_size > 0:\n",
    "        X = data[\"X\"][:data_size]\n",
    "        Y = data[\"y\"][:data_size]\n",
    "    else: \n",
    "        X = data[\"X\"]\n",
    "        Y = data[\"y\"]\n",
    "    real_skip_pairs = [x for x, y in zip(X, Y) if y]\n",
    "    dat = torch.tensor([x[0] for x in real_skip_pairs])\n",
    "    labels = torch.tensor([x[1] for x in real_skip_pairs]).long()\n",
    "    dataset = torch_data.TensorDataset(dat, labels)\n",
    "    dataloader = torch_data.DataLoader(dataset, batch_size=64, shuffle=False)     \n",
    "\n",
    "    return dataset, dataloader, word_idx_mapping\n",
    "\n",
    "\n",
    "\n",
    "data_path = './data10/skip_grams.json' # path to data \n",
    "basic_ds, basic_dataloader, basic_word_idx_mapping = load_real_skipgram_data(data_path, data_size=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size 4000\n",
      "start training emb model\n",
      "Epoch 0 finished9700 mean last 100 loss: 0.0901534479111433  \n",
      "Epoch 1 finished9700 mean last 100 loss: 0.08850483775138855 \n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(basic_word_idx_mapping)\n",
    "print('vocabulary size', vocab_size)\n",
    "net = BasicSkipGram(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "# init params\n",
    "\n",
    "    \n",
    "net.train(basic_dataloader, epochs=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create KNN\n",
      "Most Similar to three: four, five, six, seven, two\n",
      "Most Similar to cat: voice, female, showing, like, wikipedia\n",
      "Most Similar to city: route, metropolitan, canada, buffalo, boston\n",
      "Most Similar to player: league, footballer, football, professional, team\n",
      "Most Similar to king: prince, emperor, frederick, vii, khan\n",
      "Most Similar to queen: elizabeth, daughter, child, james, mary\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "class KNN():\n",
    "    \"\"\" Simple K nearest neighbour data structure \"\"\"\n",
    "    def __init__(self, embedding, word_to_idx):\n",
    "        print('Create KNN')\n",
    "        self.embedding = normalize(embedding.numpy(), axis=1)\n",
    "        self.word_to_idx = word_to_idx\n",
    "\n",
    "    def query(self, idx, k=5):\n",
    "        tmp = {}\n",
    "        for i in idx:\n",
    "            tmp[i] = self.get_most_similar(i, k)\n",
    "        return tmp\n",
    "\n",
    "    def print_nearest(self, words, k=5):\n",
    "        for x in words:\n",
    "            idx = self.word_to_idx[x]\n",
    "            k_near_idx = self.get_most_similar(idx, k)\n",
    "            similar_words = [self.word_to_idx.inverse[z] for z in k_near_idx]\n",
    "            print('Most Similar to {0}:'.format(x), ', '.join(similar_words))\n",
    "\n",
    "    def get_most_similar(self, i, k):\n",
    "        \"\"\" Get the indexes of the most similar embedding vectors \n",
    "    \n",
    "            Args:\n",
    "                i: int\n",
    "                k: int\n",
    "            Returns \n",
    "                k_nearest: list    \n",
    "        \"\"\"\n",
    "        embed_i = self.embedding[i, :].reshape(-1, 1)\n",
    "        scores = (self.embedding @ embed_i).ravel()\n",
    "        ordered_sims = np.argsort(scores)[::-1]\n",
    "        k_nearest = ordered_sims[1:k + 1] # i is probably includes\n",
    "        assert ordered_sims[0] == i\n",
    "        return k_nearest\n",
    "\n",
    "\n",
    "embedding = net.embedding\n",
    "knn = KNN(embedding, basic_word_idx_mapping)\n",
    "test_words = [\"three\", \"cat\", \"city\", \"player\", \"king\", \"queen\"]\n",
    "knn.print_nearest(test_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
